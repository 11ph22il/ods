\chapter{External Memory Searching}
\chaplabel{b-tree}

[This chapter is still under active development.  Use it at your own risk]

Throughout this book, we have been using the #w#-bit word-RAM model
of computation defined in \secref{model}.   An implicit assumption of
this model is that our computer has a large enough random access memory
to store all the data in the data structure.  In some situations, this
assumption is not valid.  There exist collections of data so big that
there is no computer with a memory large enough to store all the data.
In such cases, the application must resort to storing the data on some
external storage medium such as a hard disk, a solid state disk, or even
a network file server (which has its own external storage).

Accessing an item from the external storage is extremely slow.  The hard
disk attached to the computer on which this book is being written has
an average access time of 19ms.  The solid state drive attached to this
computer has an average access time of 0.3ms.  The random access memory in
this computer has an average access time of less than 0.000113ms.  It is
more than 2500 times faster to access RAM than the solid state drive
and more than 160000 times faster to access RAM than the hard drive.

%  HDD: Fantom ST3000DM001-9YN166 USB 3 external drive (3TB)
%  SSD: ATA OCZ-AGILITY 3 (60GB)
%  Mem: Mushkin Enhanced Essentials 4GB (2 x 2GB) 204-Pin DDR3 
%       SO-DIMM DDR3 1066 (PC3 8500) Dual Channel Kit Laptop Memory
%       Model 996643
%  Memory speed was estimated using this program:
% #include<stdlib.h>
% #include<stdio.h>
% #include<time.h>
% 
% 
% int main(void) {
%    unsigned *a, x, i, n = 50000000;
%    clock_t start, stop;
% 
%    start = clock();
%    a = malloc(sizeof(unsigned)*n);
%    for(i = 0; i < n; i++) {
%      x |= a[rand()%n];
%    }
%    stop = clock();
%    printf("x=%x, %g\n", x, (((double)(stop-start))/(double)CLOCKS_PER_SEC)/(double)n);
%    free(a);
%    return 0;
% }

These speeds are fairly typical;  accessing a random byte from from RAM
is thousands of times faster than accessing a random byte from a hard
disk or solid-state drive.  However, this does not tell the entire story.
When we access a byte from a hard disk or solid state disk, an entire
\emph{block} of the disk is read.  Each of the drives attached to
this computer has a block size of 4096; each time we read one byte,
the drive gives us a block containing 4096 bytes.  If we organize our
data structure carefully, this means that each disk access could yield
4096 bytes that are helpful in answering our query.

% morin@peewee:~/git/ods/latex$ sudo blockdev --report
% RO    RA   SSZ   BSZ   StartSec            Size   Device
% rw   256   512  4096          0     60022480896   /dev/sda   SSD
% rw   256  4096  4096        504   3000581885952   /dev/sdb1  HDD

This is the idea behind the \emph{external memory model} of computation,
illustrated schematically in \figref{em}.  In this model, the computer
has access to a large external memory where all the data resides.
This memory is divided into memory \emph{blocks} each containing $B$
words.  The computer also has a limited internal memory on which it can
perform computations.  Transferring a block between internal memory and
external memory takes constant time.  Computations performed within the
internal memory are free.  The fact that internal memory computations
are free may seem a bit strange, but this is really to just to emphasize
the fact that external memory is so much slower than RAM.

\begin{figure}
  \centering{\includegraphics{figs/em}}
  \caption{In the external memory model, accessing an individual item,
  #x#, in the external memory requires reading the entire block containing
  #x# into RAM.}
  \figlabel{em}
\end{figure}

In the full-blown external memory model, the size of the internal memory
is also a parameter.  However, for the data structures described in this
chapter, it is sufficient to have an internal memory that is capable
of storing a constant number of blocks.  In other words, to implement
these data structures it is sufficient to have an internal memory of
size $O(B)$.

In this chapter we will study search trees for external memory that
implement the #SSet# interface.  We will analyze these algorithms in
both the external memory model and in the word-RAM model. \ldots


\section{The Block Store}

The notion of external memory includes a large number of possible
different devices, each of which has their own block size and is
accessed with their own collection of system calls.  To simplify the
data structures in this chapter so that we can focus on the common ideas,
 we encapsulate external memory devices with an object called a
#BlockStore#.  A #BlockStore# stores a collection of memory blocks, each
of size $B$.  Each block is uniquely identified by its integer index.
A #BlockStore# supports these operations:

\begin{enumerate}
  \item #readBlock(i)#: Return the contents of the block whose index is #i#.

  \item #writeBlock(i,b)#: Write contents of #b# to the block whose
    index is #i#.

  \item #placeBlock(b)#: Return a new index and store the contents of #b#
    at this index.

  \item #freeBlock(i)#: Free the block whose index is #i#.  This indicates
    that the contents of this block are no longer used so the external
    memory allocated by this block may be reused.
\end{enumerate}

The easiest way to imagine a #BlockStore# is to think of it as storing a
file on disk that is partitioned into blocks, each containing $B$ bytes.
Then #readBlock(i)# and #writeBlock(i,b)# simply read and write bytes
$#i#B,\ldots,(#i#+1)B-1$ of this file.  In addition, a simple #BlockStore#
could keep a \emph{free list} of blocks that are available for use. Blocks
freed with #freeBlock(i)# are added to the free list.  In this way,
#placeBlock(b)# can use a block from the free list or, if none is
available, can append a new block to the end of the file.


\section{B-Trees}

In this section we discuss a generalization of binary trees,
called $B$-trees, that is efficient in the external memory model.
Alternatively, $B$-trees can be viewed as the natural generalization of
2-4 trees described in \secref{twofour}. (A 2-4 tree is a special case
of a $B$-tree that we get by setting $B=2$.)

For an even integer $B\ge 2$, a \emph{$B$-tree} is a tree in which
all leaves have the same depth and every non-root internal node, #u#,
has at least $B$ children and at most $2B$ children that are stored in
an array, #u.children#.  The requirement on the number of children is
relaxed at the root, which can have anywhere between 2 and $2B$ children.

If the height of a $B$-tree is $h$, then it follows that the number,
$\ell$, of leaves in the $B$-tree satisfies
\[
    2B^{h-1} \le \ell \le 2(2B)^{h-1} \enspace .
\]
Taking the logarithm of the first inequality and rearranging terms yields:
\begin{align*}
    h & \le \frac{\log \ell-1}{\log B} + 1  \\
      & \le \frac{\log \ell}{\log B} + 1 \\
      & = \log_B \ell + 1 \enspace .
\end{align*}
Each node, #u#, in $B$-tree stores an array of keys
$#u.keys#[0],\ldots,#u.keys#[2B-1]$.  If #u# is an internal node with $k$
children, then the number of keys stored at #u# is exactly $k-1$ and these
are stored in $#u.keys#[0],\ldots,#u.keys#[k-2]$.  The remaining $2B-k+1$
entries are set to #null#.  If #u# is a non-root leaf node, then #u#
contains between $B-1$ and $2B-1$ keys. The keys in a $B$-tree respect
an ordering similar to the keys in a binary search tree.  For any node,
#u#, that stores $k-1$ keys,
\[
   #u.keys[0]# < #u.keys[1]# < \cdots < #u.keys#[k-2] \enspace .
\]
If #u# is an internal node, then for every $#i#\in\{0,\ldots,k-2\}$,
$#u.keys[i]#$ is larger than every key stored in the subtree rooted at
#u.children[i]# but smaller than every key stored in the subtree rooted
at $#u.children[i+1]#$.  Informally,
\[
   #u.children[i]# \prec #u.keys[i]# \prec #u.children[i+1]# \enspace .
\]
An example of a $B$-tree with $B=2$ is shown in \figref{btree}.

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/btree-1}}
  \caption{A $B$-tree with $B=2$.}
  \figlabel{btree}
\end{figure}

Note that all the data stored in a $B$-tree node has size $O(B)$.
Therefore, in an external memory setting, the value of $B$ in a $B$-tree
is chosen so that  a node fits into a single external memory block.
In this way, the time it takes to perform a $B$-tree operation in the
external memory model is proportional to the number of nodes that are
accessed (read or written) by the operation.

For example, if the keys are 4 byte integers and the node identifiers
are also 4 bytes, then setting $B=256$ means that each node stores
\[
(4+4)\times 2B
 = 8\times512=4096
\]
bytes of data.  This would be a perfect value of $B$ for the hard disk
and solid state drive discussed in the introduction to this chaper,
which have a block size of $4096$ bytes.

The #BTree# class, which implements a $B$-tree, stores a #BlockStore#,
#bs#, that stores #BTree# nodes as well as the index, #ri#, of the
root node.  As usual, an integer, #n#, is used to keep track of the number
of items in the data structure:
\javaimport{ods/BTree.n.ri.bs}

\subsection{Searching}

The implementation of the #find(x)# operation, which is illustrated in
\figref{btree-find}, generalizes the #find(x)# operation in a binary
search tree.  The search for #x# starts at the root and uses the keys
stored at a node, #u#, to determine which of the children to of #u#
to search next.

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/btree-2}}
  \caption[Searching in a $B$-tree]{A successful search (for the value 4)
    and an unsuccessful search (for the value 16.5) in a $B$-tree. Shaded nodes show where the value of #z# is updated during the searches.}
  \figlabel{btree-find}
\end{figure}
More specifically, at a node #u#, the search checks if #x# is stored
in #u.keys#.  If so, #x# has been found and the search is complete.
Otherwise, the search finds the smallest integer, #i#, such that
$#u.keys[i]# > #x#$ and continues the search in the subtree rooted at
#u.children[i]#.  If no key in #u.keys# is greater than #x#, then the
search continues in #u#'s rightmost child.  Just like binary search
trees, the algorithm keeps track of the most recently seen key, #z#,
that is larger than #x#.  In case #x# is not found, #z# is returned as
the smallest value that is greater or equal to #x#.
\javaimport{ods/BTree.find(x)}
Central to the #find(x)# method is the #findIt(a,x)# method that searches
in a #null#-padded sorted array, #a#, for the value #x#.  This method
works for any array, #a#, where $#a#[0],\ldots,#a#[k-1]$ are a sequence
of keys in sorted order and $#a#[k],\ldots,#a#[#a.length#-1]$ are all
set to #null#.
If #x# is in the array at position #i#, then #findIt(a,x)# returns
$-#i#-1$. Otherwise, it returns the smallest index, #i#, such that
$#a[i]#>#x#$ or $#a[i]#=#null#$.
\javaimport{ods/BTree.findIt(a,x)}
The #findIt(a,x)# method does its job using a binary search that halves
the search space at each step, so it runs in $O(\log(#a.length#))$ time.
In this case $#a.length#=2B$, so #findIt(a,x)# runs in $O(\log B)$ time.

We can analyze the running time of a $B$-tree #find(x)# operation both
the usual word-RAM model (where every instruction counts) and in the
external memory model (where we only count the number of nodes accessed).
Since each leaf in a $B$-tree stores at least one key and the height
of a $B$-Tree with $\ell$ leaves is $O(\log_B\ell)$, the height of a
$B$-tree that stores #n# keys is $O(\log_B #n#)$.  Therefore, in the
external memory model, the time taken by the #find(x)# operation is
$O(\log_B #n#)$.  To determine the running time in the word-RAM model,
we have to account for the cost of calling #findIt(a,x)# for each node
we access, so the running time of #find(x)# in the word-RAM model is
\[
   O(\log_B #n#)\times O(\log B) = O(\log #n#) \enspace .
\]

\subsection{Adding}

One important difference between $B$-trees and the #BinarySearchTree#
data structure from \secref{binarysearchtree} is that the nodes of
a $B$-tree do not store pointers to their parents.  The reason for
this will be explained later.  The lack of parent pointers has the
consequence that the #add(x)# and #remove(x)# operations on $B$-trees
are most conveniently implemented as recursive methods.

Like all balanced search trees, some form of rebalancing is sometimes
required during an #add(x)# operation.  In a $B$-tree, this is done
by \emph{splitting} nodes.  Refer to \figref{btree-split} for what follows.
Although splitting takes place across two levels of recursion, it is
best understood as an operation that takes a node #u# containing $2B$
keys and having $2B+1$ children.  It creates a new node, #w#, that adopts
$#u.children#[B],\ldots,#u.children#[2B]$.  The new node #w# also takes
#u#'s $B-1$ largest keys, $#u.keys#[B],\ldots,#u.keys#[2B-1]$.  At this
point, #u# has $B$ children and $B$ keys.  The extra key, $#u.keys#[B-1]$,
is passed up to the parent of #u#, which also adopts #w#.

\begin{figure}
   \centering{\begin{tabular}{l}
     \includegraphics{figs/btree-split-1} \\[2ex]
     \multicolumn{1}{c}{$\Downarrow$} \\ [2ex]
     \includegraphics{figs/btree-split-2} \\
   \end{tabular}}
   \caption{Splitting the node #u# in a $B$-tree ($B=3$). Notice that
     the key $#u.keys#[2]=\mathrm{m}$ passes from #u# to its parent.}
   \figlabel{btree-split}
\end{figure}

The real work of the #add(x)# method is implemented by the
#addRecursive(x,ui)# method, which adds the value #x# to the subtree
whose root, #u#, has identifier #ui#.  If #u# is a leaf, then #x# is
just added into #u.keys#.  Otherwise, #x# is added recursively into the
appropriate child, $#u#'$, of #u#.  The result of this recursive call is
normally #null# but may also be a reference to a newly-created node, #w#,
that was created because $#u#'$ was split.  In this case, #u# adopts #w#
and takes its first key, completing the splitting operation on $#u#'$.

After #x# has been added (either to #u# or to a descendant of #u#),
the #addRecursive(x,ui)# method checks to see if #u# is storing too many
(more than $2B-1$) keys.  If so, then #u# needs to be \emph{split}
with a call to the #u.split()# method.  The result of calling #u.split()#,
is a new node that is used as the return value for #addRecursive(x,ui)#.
\javaimport{ods/BTree.addRecursive(x,ui)}

The #addRecursive(x,ui)# method is just a helper for the #add(x)#
method, which calls #addRecursive(x,ri)# to insert #x# into the root of
the $B$-tree.  If #addRecursive(x,ri)# causes the root to split, then
a new root is created that takes as its children both the old root and
the new node created when the old root was split.
\javaimport{ods/BTree.add(x)}
The process of adding a new value, and the resulting split operations
is illustrated in \figref{btree-add}.

\begin{figure}
   \centering{\begin{tabular}{l}
     \includegraphics[width=\ScaleIfNeeded]{figs/btree-add-1} \\[2ex]
     \multicolumn{1}{c}{$\Downarrow$} \\ [2ex]
     \includegraphics[width=\ScaleIfNeeded]{figs/btree-add-2} \\[2ex]
     \multicolumn{1}{c}{$\Downarrow$} \\ [2ex]
     \includegraphics[width=\ScaleIfNeeded]{figs/btree-add-3} 
   \end{tabular}}
   \caption{The #add(x)# operation in a #BTree#. Adding the value 21
      results in two nodes being split.}
   \figlabel{btree-add}
\end{figure}

The #add(x)# method and its helper, #addRecursive(x,ui)#, can be analyzed
in two phases:

\begin{description}
  \item[Downward phase:]
    During the downward phase of the recursion, before #x# has been added,
    they access a sequence of nodes and call #findIt(a,x)# on each node.
    Like the #find(x)# method, this takes $O(\log_B #n#)$ time is the
    external memory model and $O(\log #n#)$ time in the word-RAM model.
  
  \item[Upward phase:]
    During the upward phase of the recursion, after #x# has been added,
    these methods perform a sequence of at most $O(\log_B #n#)$ splits.
    Each split involves only 3 nodes, so this phase takes $O(\log_B #n#)$
    time in the external memory model.  However, each split involves
    moving $B$ keys and children so, in the word-RAM model, this takes
    $O(B\log #n#)$ time.
\end{description}

Recall that the value of $B$ can be quite large compared to $\log #n#$
so, in the word-RAM model, adding a value to a $B$-tree can be much
slower than adding into a balanced binary search tree.  Later, in
\secref{btree-amortized}, we will show that the situation is not quite
so bad; the amortized number of split operations done during an #add(x)#
operation is constant.  This shows that the (amortized) running time of
the #add(x)# operation in the word-RAM model is $O(B+\log #n#)$.


\subsection{Removing}

The #remove(x)# operation in a #BTree# is, again, most easily implemented
as a recursive method.  The first job of the #remove(x)# method is
to find the element #x# that should be removed.  If #x# is found in a
leaf, then #x# is removed from this leaf.  Otherwise, if #x# is found at
#u.keys[i]# for some internal node #u#, then the algorithm removes the
smallest value, #x'#, in the subtree rooted at #u.children[i+1]#. This
process is illustrated in \figref{btree-remove}.  The value #x'# is
then used to replace #x# in #u.keys[i]#.  The #removeRecursive(x,ui)#
method implements the preceding algorithm using recursion:
\begin{figure}
   \centering{\begin{tabular}{l}
     \includegraphics[width=\ScaleIfNeeded]{figs/btree-remove-1} \\[2ex]
     \multicolumn{1}{c}{$\Downarrow$} \\ [2ex]
     \includegraphics[width=\ScaleIfNeeded]{figs/btree-remove-2} 
   \end{tabular}}
   \caption{The #remove(x)# operation in a #BTree#. To remove 10 we replace
      it with the the value 11 and remove 11 from the leaf that contains it.}
   \figlabel{btree-remove}
\end{figure}
\javaimport{ods/BTree.removeRecursive(x,ui)}

Note that, after recursively removing #x# from the #i#th child of #u#,
#removeRecursive(x,ui)# needs to ensure that this child still has at
least $B-1$ keys.  In the preceding code, this is done with a call to a
method called #checkUnderflow(x,i)#.  Let #w# be the #i#th child of #u#.
If #w# has has only $B-2$ keys, then this needs to be fixed.  The fix
requires using a Note that, after recursively removing #x# from the #i#th child of #u#,
#removeRecursive(x,ui)# needs to ensure that this child still has at
least $B-1$ keys.  In the preceding code, this is done with a call to a
method called #checkUnderflow(x,i)#.  Let #w# be the #i#th
child of #u#.  If #w# has has only $B-2$ keys, then this needs to be fixed.
The fix requires using a sibling of #w#.  This can be either child $#i#+1$ of #u# or child $#i#-1$ of #u#. 


or child $#i#-1$ of #u#.  We will usually use child $#i#-1$ of #u#, which is the sibling, #v#, of #w# directly to its left.  The only time this doesn't work is when $#i#=0$, in which case we use the sibling directly to #w#'s right.
\javaimport{ods/Bree.checkUnderflow(u,i)}

 by calling yet another method:



In this case, we need to find more keys, or children, for #u#.  There are
two possible ways to do this.  If #u# has a sibling #w# with more than
$B-1$ keys, then #u# can borrow some keys (and possibly also children)
from #w#.  More specifically, if #w# stores #size(w)# keys, then between
them, #u# and #w# have a total of
\[
   B-2 + #size(w)# \ge 2B-1
\]
keys.  We can therefore shift keys from #w# to #u# so that each of #w#
and #u# has at least $B-1$ keys, thereby restoring the invariant.

If #w#'s sibling #u# has only $B-1$ children, we must do something more
drastic, since it can not afford to lend any keys to #u#.

\javaimport{ods/BTree.remove(x)}


\subsection{Amortized Analysis of $B$-Trees}
\seclabel{btree-amortized}

\section{Summary and Exercises}
\begin{exc}
  \item Explain why, in a $B$-tree, it would be a bad idea for each node
    to store a pointer to its parent node.
\end{exc}


