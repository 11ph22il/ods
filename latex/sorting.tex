\chapter{Sorting Algorithms}
\chaplabel{sorting}
\section{Merge Sort}

The #mergeSort(a)# algorithm is a classic example of recursive divide and
conquer: If the length of #a# is at most 1, then #a# is already
sorted, so we do nothing.  Otherwise, we split #a# into two halves
$#a0#=#a[0]#,\ldots,#a[n/2-1]#$ and $#a1#=#a[n/2]#,\ldots,#a[n-1]#$.
We recursively sort #a0# and #a1#, and then we merge (the now sorted)
#a0# and #a1# to get our fully sorted array #a#:
\javaimport{ods/Algorithms.mergeSort(a,c)}
Compared to sorting, merging the two sorted arrays #a0# and #a1# fairly
easy.  We add elements to #a# one at a time.  If #a0# is empty we add
the next element from #a1#.  If #a1# is empty we add the next element
from #a0#.  Otherwise, we take the minimum of the next element in #a0#
and the next element in #a1# and add it to #a#:
\javaimport{ods/Algorithms.merge(a0,a1,a,c)}
Notice that the #merge(a0,a1,a,c)# algorithm performs at most $#n#-1$
comparisons before running out of elements in one of #a0# or #a1#.

The following theorem shows that #mergeSort(a,c)# is a very efficient algorithm:
\begin{thm}
  The #mergeSort(a,c)# algorithm runs in $O(#n#\log #n#)$ time and
  performs at most $#n#\log #n#$ comparisons.
\end{thm}

\begin{proof}
The proof is by induction on $#n#$.  The base case $#n#=1$ is trivial.

Note that all comparisons done in the algorithm are done by
#merge(a0,a1,a)# during the merging phase of the algorithm.  The number of
comparisons done by #merge(a0,a1,a)# is at most $#n#-1$. If $#n#$ is even,
then we apply the inductive hypothesis to the two subproblems and obtain
\begin{align*}
  C(#n#) 
  &= #n#-1 + 2C(#n#/2) \\
  &= #n#-1 + 2((#n#/2)\log(#n#/2) \\
  &= #n#-1 + #n#\log(#n#/2) \\
  &= #n#-1 + #n#\log #n#-#n# \\
  &< #n#\log #n# \enspace .
\end{align*}
The case where $#n#$ is odd is slightly more complicated.  For this case,
we use the inequality (justified below) that shows $\log (x+1/2) <
\log x + (\log e)/2x$.  So we have
\begin{align*}
  C(#n#) 
  &= #n#-1 + C(\lceil #n#/2 \rceil) + C(\lfloor #n#/2 \rfloor) \\
  &= #n#-1 + \lceil #n#/2 \rceil\log \lceil #n#/2 \rceil 
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &= #n#-1 + (#n#/2+1/2)\log (#n#/2+1/2) 
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &< #n#-1 + \lceil #n#/2 \rceil(\log #n#/2 + (\log e)/#n#)
           + \lfloor #n#/2 \rfloor\log (#n#/2) \\
  &= #n#\log #n# - 1 + (#n#/2+1/2)(\log e/#n#) \\
  &< #n#\log #n# 
\end{align*}
since $(#n#/2+1/2)(\log e/#n#) < 1$ for all $#n#\ge 2$.

All that remains is to justify the inequality
$\log (x+1/2) <
\log x + (\log e)/2x$.  This inequality comes from the identity $\ln x = \int_1^x (1/x),\mathrm{d}x$.  This identity yields:
\[
   \ln(x+1/2) = \int_1^{x+1/2} (1/x)\,\mathrm{d}x
    = \int_1^{x} (1/x)\,\mathrm{d}x + \int_{x}^{x+1/2} (1/x)\,\mathrm{d}x
    < \int_1^{x} (1/x)\,\mathrm{d}x + 1/2x \enspace .\footnote{The same derivation yields the famous inequality, seen in many calculus books, $1+x < e^x$.}
\]
Multiplying both sides of this inequality by $\log e$ yields
$\log (x+1/2) < \log x + (\log e)/2x$.
\end{proof}

The #mergeSort(a,c)# algorithm is a classic.  It uses very few comparisons
and is quite fast.  The only drawback of this algorithm is that it uses
two additional arrays #a0# and #a1# of total size #n#.  These arrays
are allocated at run-time, which can be slow, and is a potential point
of failure if the available memory is limited.  The next two sections
discuss algorithms that are \emph{in-place}; they do all their work in
the original array #a# and don't use any auxilliary arrays.

\section{Quicksort}

The \emph{quicksort} algorithm is another classic divide and conquer
algorithm.  Unlike merge sort, which does merging after solving the two
subproblems, quick sort does all its work upfront.

The algorithm is simple to describe:  Pick a random element #x# from #a#;
partition #a# into the the set of elements less than #x#, the set of
elements equal to #x#, and the set of elements greater than #x#; and,
finally, recursively sort the first and third sets in this partition.
\javaimport{ods/Algorithms.quickSort(a,c).quickSort(a,i,n,c)}
All of this is done in-place, so that instead of making copies of
subarrays being sorted, the #quickSort(a,i,n,c)# method only sorts the
subarray $#a[i]#,\ldots,#a[i+n-1]#$.  Initially, this method is called
as #quickSort(a,0,a.length,c)#.

Quicksort is very closely related to 





\section{Heapsort}


