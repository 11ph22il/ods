\chapter{Hash Tables}
\chaplabel{hashtables}

Hash tables are an efficient method of store a small number, #n#, of
integers from a large range $U=\{0,\ldots,2^{#w#}-1\}$.   Sometimes
hash tables store data that is not integers.  In this case, an integer
\emph{hash code} is associated and this hash code is used in the hash
table.


\chaplabel{hashing}
\section{Multiplicative Hashing with Chaining}

In \emph{hashing with chaining} data is stored as an array of lists,
and we keep track of the number, #n#, of items stored in the table:
\javaimport{ods/HashTable.table.n}
The \emph{hash value} of a data item #x#, denoted #hash(x)# is a value in the range $\{0,\ldots,#table.length#\}$.  All items with hash value #i# are stored in the list at #table[i]#.
To ensure that lists don't get too long, we maintain the invariant that
\[
    #n# \le #table.length#
\]
so that the average number of elements stored in one of these lists is 
$#n#/#table.length# \le 1$.

To add an element #x# to the hash table, first check if the length
of #table# needs to be increased and, if so, we grow #table#.  With
this out of the way we hash #x# to get an integer #i# in the range
$\{0,\ldots,#table.size#-1\}$ and we append #x# to the list #table[i]#:
\javaimport{ods/HashTable.add(x)}
For any of the list implementations
described in Chapters~\ref{chap:arrays} or \ref{chap:linkedlists},
this takes only constant time.

To remove an element #x# from the hash table, we again hash it to get
an integer #i# and we remove #x# from the list #table[i]#:
\javaimport{ods/HashTable.remove(x)}
\javaimport{ods/HashTable.removeOne(x)}
This takes $O(#n#_#i#)$time, where $#n#_#i#$ is the the length of the
list stored at #table[i]#.

Searching for the element #x# in a hash table is similar.  We hash #x#
to get an integer #i# and and then perform a linear search on the list
#table[i]#:
\javaimport{ods/HashTable.find(x)}
Again, this takes time proportional to the length of the list #table[i]#.

The performance of a hash table depends critically on the choice of the
hash function.  A good hash function will spread the elements evenly
among the #table.length# lists, so that the expected size of the list
#table[hash(x)]# is roughly $O(#n#/#table.size)# = O(1)$.  On the other
hand, a bad hash function will hash all values (including #x#) to the
same table location, in which case the size of the list #table[hash(x)]#
will be #n#.  In the next section we describe a good hash function.

\subsection{Multiplicative Hashing}

Multiplicative hashing is an efficient form of hashing based on modular
arithmetic (discussed in \secref{arrayqueue}) and integer division.
It uses the $\ddiv$ operator, which does division without remainder.
For any integers $a\ge 0$ and $b\ge 1$, $a\ddiv b = \lfloor a/b\rfloor$.

In multiplicative hashing, we use a hash table of size $2^{#d#}$ for some
integer #d# (called the \emph{dimension}).  The formula for hashing an
integer $#x#\in\{0,\ldots,2^{#w#}-1\}$ is
\[
    #hash#(#x#) = ((#z#\cdot#x#) \bmod 2^{#w#}) \ddiv 2^{#w#-#d#} \enspace .
\]
Here #z# is a randomly chosen \emph{odd} integer in
$\{1,\ldots,2^{#w#}-1\}$.
This hash function can be realized very
efficiently by observing that, by default, operations on integers
are already done modulo $2^{#w#}$ where $#w#$ is the number of bits in
an integer.  (See \figref{multihashing}.) Furthermore, integer division
by $2^{#w#-#d#}$ is equivalent to dropping the rightmost $#w#-#d#$ bits in
a binary representation (which is implemented by shifting the bits
right by $#w#-#d#$).  In this way, the code that
implements the above formula is simpler than the formula itself:
\javaimport{ods/HashTable.hash(x)}

\begin{figure}
  \begin{center}
    \begin{tabular}{|lr@{}r|}\hline
    $2^#w#$ (4294967296)&            #1#&#00000000000000000000000000000000# \\
    #z# (4102541685)&                   &#11110100100001111101000101110101# \\
    #x# (42) &                          &#00000000000000000000000000101010# \\
    $#z#\cdot#x#$ &             #101000#&#00011110010010000101110100110010# \\
    $(#z#\cdot#x#)\bmod 2^{#w#}$ &      &#00011110010010000101110100110010# \\
    $((#z#\cdot#x#)\bmod 2^{#w#})\ddiv 2^{#w#-#d#}$ &&
                      \multicolumn{1}{@{}l|}{#00011110#} \\\hline
    \end{tabular}
  \end{center}
  \caption{The operation of the multiplicative hash function with $#w#=32$
    and $#d#=8$.}
  \figlabel{multihashing}
\end{figure}

The following lemma, whose proof requires some basic number theory,
shows that multiplicative hashing does a good job of avoiding collisions:

\begin{lem}\lemlabel{universal-hashing}
  Let #x# and #y# be any two values in $\{0,\ldots,2^{#w#-1}\}$ with
  $#x#\neq #y#$. Then $\Pr\{#hash(x)#=#hash(y)#\} \le 2/2^d$.
\end{lem}

With \lemref{universal-hashing}, the performance of #remove(x)#, and
#find(x)# are easily to analyze:

\begin{lem}
  For any data value #x#, the expected length of the list #table[hash(x)]#
  is at most $#n#_{#x#} + 2$, where $#n#_{#x#}$ is the number of
  occurrences of #x# in the hash table.
\end{lem}

\begin{proof}
  Let $S$ be the (multi-)set of elements stored in the hash table that
  are not equal to #x#.  For an element $#y#\in S$, define the indicator
  variable
    \[ I_{#y#} = \left\{\begin{array}{ll}
       1 & \mbox{if $#hash(x)#=#hash(y)#$} \\
       0 & \mbox{otherwise}
       \end{array}\right.
    \]
  and notice that, by \lemref{universal-hashing}, $\E[I_{#y#}] \le
  2/#table.size#$.  The expected length of the list #table[hash(x)]#
  is given by
  \begin{eqnarray*}
   \E\left[|#table[hash(x)]#|\right] &=& \E\left[#n#_{#x#} + \sum_{#y#\in S} I_{#y#}\right] \\
    &=& #n#_{#x#} + \sum_{#y#\in S} \E [I_{#y#} ] \\
    &\le& #n#_{#x#} + \sum_{#y#\in S} 2/#table.size# \\
    &\le& #n#_{#x#} + \sum_{#y#\in S} 2/#n# \\
    &\le& #n#_{#x#} + (#n#-#n#_{#x#})2/#n# \\
    &\le& #n#_{#x#} + 2 \enspace ,
  \end{eqnarray*}
  as required.
\end{proof}

Now, we want to prove \lemref{universal-hashing}, but we first give the
number theoretic tool we need:

\begin{lem}\lemlabel{hashing-mapping}
Let $S$ be the set of odd integers in $\{1,\ldots,2^{#w#}-1\}$, Let $t$
and $i$ be any two elements in $S$.  Then there is exactly one value $#z#\in
S$ such that $zt\bmod 2^{#w#} = i$.
\end{lem}

\begin{proof}
Since the number of choices for #z# and #i# are the same, it is sufficient
to prove that there is at most one value $#z#\in S$ that satisifies
$zt\bmod 2^{#w#} = i$.

Suppose, for the sake of contradiction, that there are two such values
#z# and #z'#.  Then
\[
   #z##t#\bmod 2^{#w#} = #z#'#t# \bmod 2^{#w#} = i
\]
So
\[ (#z#-#z#')#t#\bmod 2^{#w#} = 0 \]
But this means that 
\[ (#z#-#z#')#t# = #k# 2^{#w#} \]
for some integer $#k#$.  Furthermore $#k#\neq 0$ since $#t#\neq 0$ and $#z#-#z#'\neq 0$.
But this is impossible since $#t#$ is odd (so its prime factorization contains no 2's) and $|#z#-#z#'| < 2^{#w#}$ (so the prime factorization of $#z#-#z#'$ has fewer than $#w#$ 2's).
\end{proof}

The utility of \lemref{hashing-mapping} comes from the following
observation:  If #z# is chosen uniformly at random from $S$, then #zt#
is uniformly distributed over $S$.  In the following proof, it helps
to think of the binary representation of #z#, which consists of $#w#-1$
random bits followed by a 1.

\begin{proof}[Proof of \lemref{universal-hashing}]
First we note that the condition #hash(x)=hash(y)# is equivalent to
the statement ``the highest-order #d# bits of $#z##x#\bmod2^{#w#}$
and the highest-order #d# bits of $#z##y#\bmod 2^{#w#}$ are the same.''
Equivalently, the highest-order #d# bits in the binary representation
of $#z#(#x#-#y#)\bmod 2^{#w#}$ are either all 0's or all 1's:
\begin{equation}
    \underbrace{0,\ldots 0}_{#d#},\underbrace{*,\ldots,*}_{#w#-#d#} 
    \eqlabel{all-zeros}
\end{equation}
or:
\begin{equation}
    \underbrace{1,\ldots 1}_{#d#},\underbrace{*,\ldots,*}_{#w#-#d#} \enspace .
    \eqlabel{all-ones}
\end{equation}
Therefore, we only have to bound the probability that 
$#z#(#x#-#y#)\bmod 2^{#w#}$ looks like \eqref{all-zeros} or \eqref{all-ones}.

Let $#t#$ be the unique odd integer such that $#x#-#y#=#t#2^r$ for some integer
$r\ge 0$. By \lemref{hashing-mapping}, the binary representation of
$#z##t#\bmod 2^{#w#}$ has $#w#-1$ random bits, followed by a 1:
\[
 #z##t#\bmod 2^{#w#}  = \underbrace{b_{#w#-1},\ldots b_{1}}_{#w#-1},1
\]
Therefore, the binary representation of $#z#(#x#-#y#)\bmod 2^{#w#}$ has
$#w#-r-1$ random bits, followed by a 1, followed by $r$ 0's:
\[
#z#(#x#-#y#)\bmod 2^{#w#}  =
#z##t#2^{r} =
    \underbrace{b_{#w#-r-1},\ldots b_{1}}_{#w#-r-1},1,\underbrace{0,0,\ldots,0}_{r}
\]
We can now finish the proof:  If $r > #w#-#d#$, then higher order bits of
$#z#(#x#-#y#)\bmod 2^{#w#}$  contain both 0's and 1's, so the probability that
$#z#(#x#-#y#)\bmod 2^{#w#}$ looks like \eqref{all-zeros} or \eqref{all-ones}
is 0.  If $#r#=#w#-#d#$, then the probability of looking like \eqref{all-zeros}
is 0, but the probability of looking like \eqref{all-ones} is
$1/2^{#d#-1}=2/2^{#d#}$ (since we must have $b_1,\ldots,b_{d-1}=0,\ldots,0$).
If $r < #w#-#d#$ then we must have $b_{#w#-r-1},\ldots,b_{#w#-r-#d#}=0,\ldots,0$
or $b_{#w#-r-1},\ldots,b_{#w#-r-#d#}=1,\ldots,1$.  The probability of each of
these cases is $1/2^{#d#}$ and they are mutually exclusive, so the probability
of either of these cases is $2/2^{#d#}$.  This completes the proof.
\end{proof}



%\section{Perfect Hashing} (how to cope with conflicting hash code)
\section{Hash Codes}
\section{Fingerprinting}


